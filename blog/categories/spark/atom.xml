<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: spark | 研究研究]]></title>
  <link href="http://www.yanjiuyanjiu.com/blog/categories/spark/atom.xml" rel="self"/>
  <link href="http://www.yanjiuyanjiu.com/"/>
  <updated>2013-06-17T22:11:06+08:00</updated>
  <id>http://www.yanjiuyanjiu.com/</id>
  <author>
    <name><![CDATA[soulmachine]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Installing Spark on CentOS]]></title>
    <link href="http://www.yanjiuyanjiu.com/blog/20130617"/>
    <updated>2013-06-17T19:06:00+08:00</updated>
    <id>http://www.yanjiuyanjiu.com/blog/installing-spark-on-centos</id>
    <content type="html"><![CDATA[<p><strong>Environment</strong>:CentOS 6.4, Hadoop 1.1.2, JDK 1.7, Spark 0.7.2, Scala 2.9.3</p>

<p>After a few days hacking , I have found that installing a Spark cluster is exteremely easy :)</p>

<h1 id="install-jdk-17">1. Install JDK 1.7.</h1>
<pre><code>yum search openjdk-devel
sudo yum install java-1.7.0-openjdk-devel.x86_64
/usr/sbin/alternatives --config java
/usr/sbin/alternatives --config javac
sudo vim /etc/profile
# add the following lines at the end
export JAVA_HOME=/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.19.x86_64
export JRE_HOME=$JAVA_HOME/jre
export PATH=$PATH:$JAVA_HOME/bin
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
# save and exit vim
# make environment variables take effect immediately
$ source /etc/profile
# test
$ java -version
</code></pre>

<h1 id="install-scala-293">2. Install Scala 2.9.3</h1>
<p>Spark 0.7.2 depends on Scala 2.9.3, So we must install Scala of version 2.9.3.</p>

<p>Download <a href="http://www.scala-lang.org/downloads/distrib/files/scala-2.9.3.tgz">scala-2.9.3.tgz</a> and save it to home directory.</p>

<pre><code>$ tar -zxf scala-2.9.3.tgz
$ sudo mv scala-2.9.3 /usr/lib
$ sudo vim /etc/profile
# add the following lines at the end
export SCALA_HOME=/usr/lib/scala-2.9.3
export PATH=$PATH:$SCALA_HOME/bin
# save and exit vim
# make environment variables take effect immediately
source /etc/profile
# test
$ scala -version
</code></pre>

<h1 id="download-prebuilt-packages">3. Download prebuilt packages</h1>
<p>Download prebuilt packages, <a href="http://www.spark-project.org/download-spark-0.7.2-prebuilt-hadoop1">spark-0.7.2-prebuilt-hadoop1.tgz</a>. </p>

<p>If you want to compile it from scratch, download the source package, but I don’t recommend this way, because in Chinese Mainland the GFW has blocked one of maven repositories, twitter4j.org, which makes the compilation an impossible mission unless you can conquer GFW.</p>

<h1 id="local-mode">4. Local Mode</h1>

<h2 id="untar-the-tarball-and-set-sparkhome">4.1 Untar the tarball and set <code>SPARK_HOME</code></h2>

<pre><code>$ tar -zxf spark-0.7.2-prebuilt-hadoop1.tgz
$ vim ~/.bash_profile
# add the following lines at the end
export SPARK_HOME=$HOME/spark-0.7.2
export PATH=$PATH:$SPARK_HOME/bin
# save and exit vim
# make environment variables take effect immediately
$ source /etc/profile
</code></pre>

<h2 id="set-the-sparkexamplesjar-environment-variable">4.2 Set the <code>SPARK_EXAMPLES_JAR</code> environment variable</h2>
<pre><code>$ vim ~/.bash_profile
# add the following lines at the end
export SPARK_EXAMPLES_JAR=$SPARK_HOME/examples/target/scala-2.9.3/spark-examples_2.9.3-0.7.2.jar
# save and exit vim
# make environment variables take effect immediately
$ source /etc/profile
</code></pre>

<p>This is the most important step that must be done , but unfortunately the official docs and most web blogs haven’t mentioned this. I found this step when I bumped into these posts, <a href="https://groups.google.com/forum/?fromgroups#!topic/spark-users/nQ6wB2lcFN8">Running SparkPi</a>, <a href="https://groups.google.com/forum/#!msg/spark-users/x5UczgI-Xm8/wzMm3Mb77-oJ">Null pointer exception when running ./run spark.examples.SparkPi local</a>.</p>

<h2 id="now-you-can-run-sparkpi">4.3 Now you can run SparkPi.</h2>

<pre><code>$ cd ~/spark-0.7.2
$ ./run spark.examples.SparkPi local 
</code></pre>

<h1 id="cluster-mode">5. Cluster Mode</h1>

<!-- more -->

<h2 id="install-hadoop">5.1 Install hadoop</h2>
<p>Use VMware Workstation to create three CentOS virtual machines, which’s hostnames are master, slave01, slave02, setup password-less ssh to the slaves, install hadoop on the three machines and start up the hadoop cluster. For more details please read another blog of mine, <a href="http://www.yanjiuyanjiu.com/blog/20130612">在CentOS上安装Hadoop</a>.</p>

<h2 id="install-jdk-and-scala">5.2 Install JDK and Scala</h2>
<p>Install JDK 1.7 and Scala 2.9.3 on the three machines, according to section 1 and section 2.</p>

<h2 id="install-and-configure-spark-on-master">5.3 Install and configure Spark on master</h2>
<pre><code>$ tar -zxf spark-0.7.2-prebuilt-hadoop1.tgz
$ vim ~/.bash_profile
# add the following lines at the end
export SPARK_HOME=$HOME/spark-0.7.2
export PATH=$PATH:$SPARK_HOME/bin
export SPARK_EXAMPLES_JAR=$SPARK_HOME/examples/target/scala-2.9.3/spark-examples_2.9.3-0.7.2.jar
# save and exit vim
# make environment variables take effect immediately
$ source /etc/profile
</code></pre>

<p>Set <code>SCALA_HOME</code> in <code>conf/spark-env.sh</code></p>

<pre><code>$ cd $SPARK_HOME/conf
$ mv spark-env.sh.template spark-env.sh
$ vim spark-env.sh
export SCALA_HOME=/usr/lib/scala-2.9.3
# save and exit
</code></pre>

<p>In<code>conf/slaves</code>, add hostnames of Spark workers, one per line.</p>

<pre><code>$ vim slaves
slave01
slave02
# save and exit
</code></pre>

<h2 id="install-and-configure-spark-on-slaves">5.4 Install and configure Spark on slaves</h2>
<p>Copy the spark directory to all slaves</p>

<pre><code>scp -r spark-0.7.2 dev@slave01:~
scp -r spark-0.7.2 dev@slave02:~
</code></pre>

<p>Set environment variables on all slaves as section 5.3.</p>

<h2 id="start-spark-cluster">5.5 Start Spark cluster</h2>
<p>On master</p>

<pre><code>$ cd $SPARK_HONE
$ bin/start-all.sh
</code></pre>

<p>Check whether the processes have been started.</p>

<pre><code>$ jps
11055 Jps
2313 SecondaryNameNode
2409 JobTracker
2152 NameNode
4822 Master
</code></pre>

<p>Look at the master’s web UI (<a href="http://localhost:8080">http://localhost:8080</a> by default). You should see the new node listed there, along with its number of CPUs and memory (minus one gigabyte left for the OS).</p>

<h2 id="run-the-sparkpi-example-in-cluster-mode">5.6 run the SparkPi example in cluster mode</h2>

<pre><code>$ cd $SPARK_HONE
$ ./run spark.examples.SparkPi spark://master:7077
</code></pre>

<h2 id="read-files-from-hdfs-and-run-wordcount">5.7 read files from HDFS and run WordCount</h2>

<pre><code>$ cd $SPARK_HOME
$ hadoop fs -put README.md .
$ ./spark-shell
scala&gt; val file = sc.textFile("hdfs://master:9000/user/dev/README.md")
scala&gt; val count = file.flatMap(line =&gt; line.split(" ")).map(word =&gt; (word, 1)).reduceByKey(_+_)
scala&gt; count.collect()
</code></pre>

<h2 id="stop-spark-cluster">5.8 Stop Spark cluster</h2>

<pre><code>$ cd $SPARK_HONE
$ bin/stop-all.sh
</code></pre>

<h1 id="references">References</h1>
<ol>
  <li><a href="http://spark-project.org/docs/latest/spark-standalone.html">Spark Standalone Mode</a></li>
  <li><a href="https://github.com/mesos/spark/wiki/Running-A-Spark-Standalone-Cluster">Running A Spark Standalone Cluster</a></li>
  <li><a href="http://sprism.blogspot.com/2012/11/lightning-fast-wordcount-using-spark.html">Lightning-Fast WordCount using Spark Alongside Hadoop</a></li>
</ol>

<p>The following posts are outdated.</p>

<ol>
  <li><a href="http://chapeau.freevariable.com/2013/04/installing-spark-on-fedora-18.html">Installing Spark on Fedora 18</a></li>
  <li><a href="http://rdc.taobao.com/team/jm/archives/1823">Spark随谈（二）—— 安装攻略</a></li>
  <li><a href="http://www.cnblogs.com/jerrylead/archive/2012/08/13/2636115.html">Spark安装与学习</a></li>
</ol>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[使用Scala IDE 阅读spark源码 -- 将sbt项目转化为eclipse项目]]></title>
    <link href="http://www.yanjiuyanjiu.com/blog/20130611"/>
    <updated>2013-06-11T11:48:00+08:00</updated>
    <id>http://www.yanjiuyanjiu.com/blog/read-spark-source-code-using-scala-ide</id>
    <content type="html"><![CDATA[<p>阅读Spark源代码，最简单的方式是下载源码包，解压后用纯文本方式来阅读源码。这样效率不高，可以用sbteclipse这个插件，将sbt项目文件转化为eclipse项目文件，然后导入到Scala IDE，用eclipse来阅读源码，效率大大提高。</p>

<p><strong>环境</strong>：Windows 7, JDK 1.6</p>

<ol>
  <li>安装 scala。去官网 <a href="http://www.scala-lang.org/">http://www.scala-lang.org/</a> ，下载MSI，安装，按默认设置即可。</li>
  <li>
    <p>安装 sbt。去官网 <a href="http://www.scala-sbt.org/">http://www.scala-sbt.org/</a> ， 下载MSI，安装，按默认设置即可。
Linux下可以省略以上两步，spark源码自带了一个sbt，且启动sbt时它会自动下载对应的scala编译器。</p>
  </li>
  <li>
    <p>安装 Scala IDE。 去官网 <a href="http://scala-ide.org/">http://scala-ide.org/</a>，点击”Get the SDK”绿色按钮，下载。这个IDE的好处是，自带了scala编译器，解压即可使用。</p>
  </li>
  <li>
    <p>下载spark源码。 去官网 <a href="http://spark-project.org/">http://spark-project.org/</a> 下载源码，当前版本是 0.7.2, source package 大约4M左右。解压源码，例如 解压到 d:spark-0.7.0\</p>
  </li>
  <li>
    <p>添加 sbteclipse 插件依赖。spark已经添加了依赖，这一步什么也不需要做。</p>

    <p>这个插件的作用，就是能够读取sbt的配置文件，生成一个eclipse的工程文件。有了eclipse工程文件，就可以导入到eclipse了。</p>

    <p>spark已经添加了依赖，见 d:\spark-0.7.2\project\plugins.sbt，有一行</p>

    <p>addSbtPlugin(“com.typesafe.sbteclipse” % “sbteclipse-plugin” % “2.1.1”)</p>
  </li>
  <li>
    <p>启动cmd，启动sbt。</p>

    <blockquote>
      <p>cd  d:\spark-0.7.0<br />
 sbt</p>
    </blockquote>

    <p>Linux下则是</p>

    <blockquote>
      <p>cd  d:\spark-0.7.0<br />
 sbt/sbt</p>
    </blockquote>

    <p>开始下载各种依赖包，需要等待很长时间。</p>
  </li>
  <li>
    <p><strong>翻@_@墙。见本文最后一段。</strong></p>

    <p><!--more--></p>
  </li>
  <li>
    <p>等sbt提升符&gt;出现后，输入eclipse命令，开始生成eclipse工程文件，需要耐心等待一段时间</p>

    <blockquote>
      <p>&gt;eclipse<br />
 [info] About to create Eclipse project files for your project(s).<br />
 [info] Successfully created Eclipse project files for project(s):<br />
 [info] spark-examples  <br />
 [info] spark-streaming <br />
 [info] spark-repl<br />
 [info] spark-bagel<br />
 [info] spark-core  </p>
    </blockquote>

    <p>这样就生成成功了，去core, bagel, streaming, repl, examples 五个文件夹下，可以看到有一个.project和.classpath文件。从这里也可以看出，spark源码由五个项目组成。</p>
  </li>
  <li>用 Scala IDE 导入这5个工程，选择 d:\spark-0.7.2 文件夹，可以一次性导入5个项目。
项目图标上有红色感叹号，是因为jar包的路径不对，右击某个项目，选择 “Build Path -&gt; Configure Build Path” ，删除所有的jar，点击 “Add external jars”，浏览到 d:\spark-0.7.2\lib_managed\jars，添加所有的jar，这是红色感叹号就消失了。每个项目都如此操作一番。</li>
</ol>

<p><strong>注意：第8步在国内是无法成功的，因为一些maven仓库被墙，例如 twitter4j.org这个仓库就被墙了。因此需要翻@_@墙。</strong></p>

<p>我平时用goagent翻@_@墙，不过goagent只能让浏览器翻@_@墙，如何让goagent变成全局代理呢？即所有http协议都经过goagent。可以用 Proxifier，它可以把goagent变成操作系统全局的http代理。</p>

<p>不过 spark 在访问maven仓库时，用的是https网址，即https协议，虽然goagent可以用来访问https页面，但 goagent 和 Proxifier 使用时，https协议总是链接不通（参考 <a href="https://code.google.com/p/goagent/issues/detail?id=5210">https://code.google.com/p/goagent/issues/detail?id=5210</a>）。</p>

<p>于是我又想到了另一个方法，用SSH翻@_@墙。去网上找一个免费的ssh，安装 Bitvise SSH Client，然后在”Services”标签页面，勾选”SOCKS/HTTP Proxy Forwarding”，这样来翻@_@墙，Proxifier  使用  Bitvise SSH Client 提供的代理。</p>

<p>翻@_@墙成功后，再输入 eclipse，当到达 twitter4j.org 时，会发现 SUCCESS了。耐心等待，最后会成功生成.project文件。</p>

<p>用Scala IDE 导入项目，就可以开始阅读spark 源码了 :)</p>
]]></content>
  </entry>
  
</feed>
